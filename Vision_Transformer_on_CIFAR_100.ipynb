{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdo-dAngxHg"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZMc8gRtKn1_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "A04SOnMyh2VP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "WTuxBtXuiXn7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.1.0+cu121'"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "aBHKspSDiaPr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'0.16.0+cu121'"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torchvision.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow7RZTEEida8"
      },
      "source": [
        "## 2. Setup Device-Agnostic Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "SF8aTEoOivX7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Jun  9 15:20:07 2025       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:65:00.0 Off |                  N/A |\n",
            "| 36%   35C    P8              21W / 350W |   1966MiB / 24576MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA GeForce RTX 3090        Off | 00000000:B3:00.0  On |                  N/A |\n",
            "| 36%   34C    P8              35W / 350W |    749MiB / 24576MiB |     37%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      1428      G   /usr/lib/xorg/Xorg                            4MiB |\n",
            "|    0   N/A  N/A      4905      G   /usr/lib/xorg/Xorg                            4MiB |\n",
            "|    0   N/A  N/A     59729      C   ...e/iiit/anaconda3/envs/a1/bin/python     1944MiB |\n",
            "|    1   N/A  N/A      1428      G   /usr/lib/xorg/Xorg                           35MiB |\n",
            "|    1   N/A  N/A      4905      G   /usr/lib/xorg/Xorg                          214MiB |\n",
            "|    1   N/A  N/A      5031      G   /usr/bin/gnome-shell                         36MiB |\n",
            "|    1   N/A  N/A      9356      G   ...erProcess --variations-seed-version      205MiB |\n",
            "|    1   N/A  N/A     11936      G   /usr/lib/firefox/firefox                    206MiB |\n",
            "|    1   N/A  N/A     49124      G   ...gnu/webkit2gtk-4.0/WebKitWebProcess       11MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Qkpz-xDti3WH"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 102,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Z2K1YP8Birlp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "NuJkytgXi9W-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Device: cuda\n"
          ]
        }
      ],
      "source": [
        "print(f\"Using Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLc4LurBjAdi"
      },
      "source": [
        "## 3. Set the seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "XQONzskujETy"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6UbfaCNjLJr"
      },
      "source": [
        "## 4. Setting the hyperprameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "CteELobGjdry"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0003"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "3e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "GEzLdAQDjPL9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 30 # Try increasing epochs to 30\n",
        "LEARNING_RATE = 3e-4\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 100\n",
        "IMAGE_SIZE = 32 # Transform the image and make the size go to 224\n",
        "CHANNELS = 3\n",
        "EMBED_DIM = 192\n",
        "NUM_HEADS = 3# INcrease the number heads\n",
        "DEPTH = 6\n",
        "MLP_DIM = 512\n",
        "DROP_RATE = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdC_ItGfj4zk"
      },
      "source": [
        "## 5. Define Image Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "OYdDnLhnj92T"
      },
      "outputs": [],
      "source": [
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHsqCOJGkZdJ"
      },
      "source": [
        "## 6. Getting a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root='data', train=True, transform=transform_train, download=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset=train_dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_loader:\n",
        "    print(x.shape)  # Should be: torch.Size([BATCH_SIZE, 3, 224, 224])\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(32),  # Match model's expected input size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "test_dataset = datasets.CIFAR100(root=\"data\",\n",
        "                                 train=False,\n",
        "                                 download=True,\n",
        "                                 transform=transform_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loader = DataLoader(dataset=test_dataset,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "G7ctndZflCNa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset CIFAR100\n",
              "    Number of datapoints: 50000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               RandomCrop(size=(32, 32), padding=4)\n",
              "               RandomHorizontalFlip(p=0.5)\n",
              "               ColorJitter(brightness=(0.8, 1.2), contrast=(0.8, 1.2), saturation=(0.8, 1.2), hue=(-0.2, 0.2))\n",
              "               ToTensor()\n",
              "               Normalize(mean=0.5, std=0.5)\n",
              "           )"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "UGZgEMONlD-H"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset CIFAR100\n",
              "    Number of datapoints: 10000\n",
              "    Root location: data\n",
              "    Split: Test\n",
              "    StandardTransform\n",
              "Transform: Compose(\n",
              "               Resize(size=32, interpolation=bilinear, max_size=None, antialias=warn)\n",
              "               ToTensor()\n",
              "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
              "           )"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Xd1-TAF2lMdl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "Ft8U0i5alQDk"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwHqD3_AlRKh"
      },
      "source": [
        "## 7. Converting our datasets into dataloaders\n",
        "\n",
        "Right now, our data is in the form of PyTorch Datasets.\n",
        "\n",
        "DataLoader turns our data into batches or (mini-batches)\n",
        "\n",
        "Why would we do this?\n",
        "\n",
        "1. It is more computationally efficient, as in, your computing hardware may not be able to look (store in memroy) at 50000 images in one hit. So we break it into 128 images at a time. (batch size of 128).\n",
        "2. It gives our neural network more chances to update its gradients per epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "Q8mb4zHimKYl"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "e6YmFJWCmUz3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataLoader: (<torch.utils.data.dataloader.DataLoader object at 0x7fe883513580>, <torch.utils.data.dataloader.DataLoader object at 0x7fe883512f80>)\n",
            "Length of train_loader: 391 batches of 128...\n",
            "Length of test_loader: 79 batches of 128...\n"
          ]
        }
      ],
      "source": [
        "# Let's check out what we've created\n",
        "print(f\"DataLoader: {train_loader, test_loader}\")\n",
        "print(f\"Length of train_loader: {len(train_loader)} batches of {BATCH_SIZE}...\")\n",
        "print(f\"Length of test_loader: {len(test_loader)} batches of {BATCH_SIZE}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 3, 32, 32])\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_loader:\n",
        "    print(x.shape)  # Expect: (batch_size, 3, 224, 224)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "TcpDXQltmjpJ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50000.0"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "128 * 390.625"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "imq29xDNmrCu"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "390.625"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "50000 / 128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvYIfIHWm01X"
      },
      "source": [
        "## 8. Building Vision Transformer Model From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "KKZf3iS_nL0V"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PATCH_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=32,\n",
        "                 patch_size=4,\n",
        "                 in_channels=3,\n",
        "                 embed_dim=192):  \n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_channels=in_channels,\n",
        "                              out_channels=embed_dim,\n",
        "                              kernel_size=patch_size,\n",
        "                              stride=patch_size)\n",
        "        \n",
        "        num_patches = (img_size // patch_size) ** 2  # (32 / 4)^2 = 64\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))  # 1 + 64 = 65\n",
        "        \n",
        "        \n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        B = x.size(0)\n",
        "        x = self.proj(x)                           # Shape: (B, E, H/P, W/P)\n",
        "        x = x.flatten(2).transpose(1, 2)           # Shape: (B, N, E)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)      # Shape: (B, 1+N, E)\n",
        "        x = x + self.pos_embed                     # Positional embedding added\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "1_w3VPKZqOvf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'torch.nn.functional' from '/home/iiit/anaconda3/envs/a1/lib/python3.10/site-packages/torch/nn/functional.py'>"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "XJKzSpthpAQa"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_features,\n",
        "                 hidden_features,\n",
        "                 drop_rate):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features=in_features,\n",
        "                             out_features=hidden_features)\n",
        "        self.fc2 = nn.Linear(in_features=hidden_features,\n",
        "                             out_features=in_features)\n",
        "        self.dropout = nn.Dropout(drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.gelu(self.fc1(x)))\n",
        "        x = self.dropout(self.fc2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "zcRUvJoSqo4K"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, mlp_dim, drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "F0K8g_XerueW"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        self.encoder = nn.Sequential(*[\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = self.encoder(x)\n",
        "        x = self.norm(x)\n",
        "        cls_token = x[:, 0]\n",
        "        return self.head(cls_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 65, 192])\n"
          ]
        }
      ],
      "source": [
        "print(model.patch_embed.pos_embed.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "w69xQRmxtjb_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 133,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEPTH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "xbIFpsP0txLr"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 134,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "Lvv6XQJjtQpS"
      },
      "outputs": [],
      "source": [
        "# Instantiate model\n",
        "model = VisionTransformer(\n",
        "    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,\n",
        "    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "VhjVXjpCH6ky"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VisionTransformer(\n",
              "  (patch_embed): PatchEmbedding(\n",
              "    (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
              "  )\n",
              "  (encoder): Sequential(\n",
              "    (0): TransformerEncoderLayer(\n",
              "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=192, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=192, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): TransformerEncoderLayer(\n",
              "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=192, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=192, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (2): TransformerEncoderLayer(\n",
              "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=192, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=192, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (3): TransformerEncoderLayer(\n",
              "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=192, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=192, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (4): TransformerEncoderLayer(\n",
              "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=192, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=192, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (5): TransformerEncoderLayer(\n",
              "      (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (attn): MultiheadAttention(\n",
              "        (out_proj): NonDynamicallyQuantizableLinear(in_features=192, out_features=192, bias=True)\n",
              "      )\n",
              "      (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "      (mlp): MLP(\n",
              "        (fc1): Linear(in_features=192, out_features=512, bias=True)\n",
              "        (fc2): Linear(in_features=512, out_features=192, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
              "  (head): Linear(in_features=192, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe-MuP-5If-S"
      },
      "source": [
        "## 9. Defining a Loss function and an optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "qHJjtwUJIm0D"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss() # Measure how wrong our model is\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), # update our model's parameters to try and reduce the loss\n",
        "                             lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "FgfZMOwuIy2T"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "TQHuBb8_I06p"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.0003\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2vr-OiWI2Gc"
      },
      "source": [
        "## 10. Defining a Training Loop function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "Gg2SRUoCJZmp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlWtLw7rI_YV"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, optimizer, criterion):\n",
        "    # Set the mode of the model into training\n",
        "    model.train()\n",
        "\n",
        "    total_loss, correct = 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        # Moving (Sending) our data into the target device\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        # 1. Forward pass (model outputs raw logits)\n",
        "        out = model(x)\n",
        "        # 2. Calcualte loss (per batch)\n",
        "        loss = criterion(out, y)\n",
        "        # 3. Perform backpropgation\n",
        "        loss.backward()\n",
        "        # 4. Perforam Gradient Descent\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "        correct += (out.argmax(1) == y).sum().item()\n",
        "   \n",
        "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x)\n",
        "            correct += (preds.argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return 100 * correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "2754URTLRZQt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 144,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "EPOCHS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 65, 192])\n"
          ]
        }
      ],
      "source": [
        "print(model.patch_embed.pos_embed.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch shape: torch.Size([128, 3, 32, 32])\n",
            "Positional embedding shape: torch.Size([1, 65, 192])\n",
            "Patch embed output shape: torch.Size([128, 65, 192])\n"
          ]
        }
      ],
      "source": [
        "# Quick debug after your model definition and DataLoader setup\n",
        "x, y = next(iter(train_loader))\n",
        "print(\"Input batch shape:\", x.shape)  # Expected: [batch_size, 3, 32, 32]\n",
        "print(\"Positional embedding shape:\", model.patch_embed.pos_embed.shape)\n",
        "\n",
        "# Include a forward pass through only the patch embedding:\n",
        "out = model.patch_embed(x.to(device))\n",
        "print(\"Patch embed output shape:\", out.shape)  # Should be [batch_size, 65, 192]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rebuild the model and optimizer\n",
        "model = VisionTransformer(\n",
        "    img_size=32,\n",
        "    patch_size=4,\n",
        "    in_channels=3,\n",
        "    num_classes=100,\n",
        "    embed_dim=192,\n",
        "    depth=6,\n",
        "    num_heads=3,\n",
        "    mlp_dim=512,\n",
        "    drop_rate=0.1\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "ZprlkR56SJUb"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "ZVwtTFN0RIg7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 1/30 [00:45<22:12, 45.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1/30, Train loss: 4.2227, Train acc: 0.0563%, Test acc: 9.2300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 2/30 [01:31<21:26, 45.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2/30, Train loss: 3.8524, Train acc: 0.1069%, Test acc: 14.8800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 3/30 [02:18<20:45, 46.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3/30, Train loss: 3.6530, Train acc: 0.1408%, Test acc: 18.7200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 4/30 [03:04<20:00, 46.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4/30, Train loss: 3.5068, Train acc: 0.1635%, Test acc: 20.7500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 5/30 [03:49<19:05, 45.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5/30, Train loss: 3.3642, Train acc: 0.1921%, Test acc: 23.4200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 6/30 [04:32<17:53, 44.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6/30, Train loss: 3.2526, Train acc: 0.2106%, Test acc: 23.9000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 7/30 [05:15<16:57, 44.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7/30, Train loss: 3.1532, Train acc: 0.2291%, Test acc: 26.5200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 8/30 [06:01<16:26, 44.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8/30, Train loss: 3.0569, Train acc: 0.2485%, Test acc: 29.2300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 9/30 [06:48<15:52, 45.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9/30, Train loss: 2.9742, Train acc: 0.2628%, Test acc: 29.4200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 10/30 [07:34<15:13, 45.69s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10/30, Train loss: 2.8979, Train acc: 0.2788%, Test acc: 31.2200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 11/30 [08:20<14:32, 45.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11/30, Train loss: 2.8199, Train acc: 0.2920%, Test acc: 32.0000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 12/30 [09:07<13:47, 45.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 12/30, Train loss: 2.7552, Train acc: 0.3066%, Test acc: 33.2500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 13/30 [09:53<13:04, 46.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13/30, Train loss: 2.6902, Train acc: 0.3171%, Test acc: 35.3700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 14/30 [10:40<12:20, 46.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 14/30, Train loss: 2.6186, Train acc: 0.3328%, Test acc: 36.0300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 15/30 [11:26<11:35, 46.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15/30, Train loss: 2.5596, Train acc: 0.3454%, Test acc: 36.8100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 16/30 [12:13<10:49, 46.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 16/30, Train loss: 2.4967, Train acc: 0.3585%, Test acc: 38.0900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 17/30 [12:59<10:00, 46.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17/30, Train loss: 2.4348, Train acc: 0.3711%, Test acc: 38.2500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 18/30 [13:44<09:11, 45.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 18/30, Train loss: 2.3766, Train acc: 0.3823%, Test acc: 39.0800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 19/30 [14:30<08:25, 45.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19/30, Train loss: 2.3092, Train acc: 0.3955%, Test acc: 40.8600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 20/30 [15:16<07:40, 46.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 20/30, Train loss: 2.2549, Train acc: 0.4055%, Test acc: 42.2100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 21/30 [16:02<06:53, 45.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21/30, Train loss: 2.1972, Train acc: 0.4201%, Test acc: 42.9600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 22/30 [16:48<06:07, 45.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 22/30, Train loss: 2.1416, Train acc: 0.4306%, Test acc: 42.2900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 77%|███████▋  | 23/30 [17:33<05:19, 45.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 23/30, Train loss: 2.0848, Train acc: 0.4456%, Test acc: 43.6300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 24/30 [18:18<04:34, 45.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 24/30, Train loss: 2.0400, Train acc: 0.4533%, Test acc: 44.1600\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 25/30 [19:04<03:48, 45.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 25/30, Train loss: 1.9922, Train acc: 0.4644%, Test acc: 45.2700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 87%|████████▋ | 26/30 [19:50<03:03, 45.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 26/30, Train loss: 1.9471, Train acc: 0.4738%, Test acc: 44.7900\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 27/30 [20:36<02:17, 45.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 27/30, Train loss: 1.9177, Train acc: 0.4781%, Test acc: 45.8000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 28/30 [21:22<01:31, 45.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 28/30, Train loss: 1.8785, Train acc: 0.4867%, Test acc: 46.1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 29/30 [22:08<00:46, 46.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 29/30, Train loss: 1.8369, Train acc: 0.4974%, Test acc: 47.0400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [22:54<00:00, 45.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 30/30, Train loss: 1.7956, Train acc: 0.5076%, Test acc: 47.3700\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "### Training\n",
        "train_accuracies, test_accuracies = [], []\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "    print(f\"Epoch: {epoch+1}/{EPOCHS}, Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f}%, Test acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "HtfzazjLUVo7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0563,\n",
              " 0.10692,\n",
              " 0.1408,\n",
              " 0.16354,\n",
              " 0.19206,\n",
              " 0.2106,\n",
              " 0.22914,\n",
              " 0.24846,\n",
              " 0.26284,\n",
              " 0.27882,\n",
              " 0.29202,\n",
              " 0.3066,\n",
              " 0.31708,\n",
              " 0.33284,\n",
              " 0.34544,\n",
              " 0.35848,\n",
              " 0.37114,\n",
              " 0.38226,\n",
              " 0.39548,\n",
              " 0.40552,\n",
              " 0.42008,\n",
              " 0.43062,\n",
              " 0.44564,\n",
              " 0.45326,\n",
              " 0.46444,\n",
              " 0.47376,\n",
              " 0.47806,\n",
              " 0.48672,\n",
              " 0.49744,\n",
              " 0.50762]"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "h26UKVzBUbcI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[9.23,\n",
              " 14.88,\n",
              " 18.72,\n",
              " 20.75,\n",
              " 23.42,\n",
              " 23.9,\n",
              " 26.52,\n",
              " 29.23,\n",
              " 29.42,\n",
              " 31.22,\n",
              " 32.0,\n",
              " 33.25,\n",
              " 35.37,\n",
              " 36.03,\n",
              " 36.81,\n",
              " 38.09,\n",
              " 38.25,\n",
              " 39.08,\n",
              " 40.86,\n",
              " 42.21,\n",
              " 42.96,\n",
              " 42.29,\n",
              " 43.63,\n",
              " 44.16,\n",
              " 45.27,\n",
              " 44.79,\n",
              " 45.8,\n",
              " 46.1,\n",
              " 47.04,\n",
              " 47.37]"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "PA7BiIGfUeIS"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot accuracy\n",
        "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
        "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.title(\"Training and Test Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "gHMLNwbIVz9Q"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "saTYEqzdWK9J"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "execution_count": 154,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "WHHVI1RrWdbZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 3, 32, 32])"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[0][0].unsqueeze(dim=0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "1EYbt9fnW7is"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.7804, 0.7686, 0.7647,  ..., 0.8471, 0.8510, 0.8784],\n",
              "         [0.7725, 0.7647, 0.7647,  ..., 0.9059, 0.9137, 0.9451],\n",
              "         [0.8706, 0.8353, 0.8196,  ..., 0.9529, 0.9569, 0.9804],\n",
              "         ...,\n",
              "         [0.2824, 0.2784, 0.2902,  ..., 0.8627, 0.7176, 0.6078],\n",
              "         [0.2824, 0.2980, 0.3294,  ..., 0.8706, 0.7333, 0.5686],\n",
              "         [0.3137, 0.3294, 0.3333,  ..., 0.8510, 0.8118, 0.6902]],\n",
              "\n",
              "        [[0.8431, 0.8275, 0.8235,  ..., 0.9059, 0.9059, 0.9176],\n",
              "         [0.8235, 0.8157, 0.8235,  ..., 0.9529, 0.9529, 0.9608],\n",
              "         [0.8863, 0.8627, 0.8588,  ..., 0.9804, 0.9765, 0.9804],\n",
              "         ...,\n",
              "         [0.2863, 0.2902, 0.3059,  ..., 0.8157, 0.6588, 0.5529],\n",
              "         [0.2941, 0.3176, 0.3490,  ..., 0.8314, 0.6824, 0.5176],\n",
              "         [0.3333, 0.3529, 0.3608,  ..., 0.8118, 0.7608, 0.6431]],\n",
              "\n",
              "        [[0.9765, 0.9569, 0.9529,  ..., 0.9804, 0.9804, 0.9882],\n",
              "         [0.9373, 0.9333, 0.9412,  ..., 0.9804, 0.9804, 0.9922],\n",
              "         [0.9647, 0.9490, 0.9529,  ..., 0.9843, 0.9843, 0.9922],\n",
              "         ...,\n",
              "         [0.3882, 0.4000, 0.4235,  ..., 0.8510, 0.7098, 0.5882],\n",
              "         [0.4078, 0.4353, 0.4784,  ..., 0.8627, 0.7529, 0.5843],\n",
              "         [0.4627, 0.4824, 0.4980,  ..., 0.8431, 0.8275, 0.7176]]])"
            ]
          },
          "execution_count": 156,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset[0][0] / 2 + 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "sN-g0LsEVXq4"
      },
      "outputs": [],
      "source": [
        "def predict_and_plot_grid(model,\n",
        "                          dataset,\n",
        "                          classes,\n",
        "                          grid_size=3):\n",
        "    model.eval()\n",
        "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(9, 9))\n",
        "    for i in range(grid_size):\n",
        "        for j in range(grid_size):\n",
        "            idx = random.randint(0, len(dataset) - 1)\n",
        "            img, true_label = dataset[idx]\n",
        "            input_tensor = img.unsqueeze(dim=0).to(device)\n",
        "            with torch.inference_mode():\n",
        "                output = model(input_tensor)\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "            img = img / 2 + 0.5 # Unormalize our images to be able to plot them with matplotlib\n",
        "            npimg = img.cpu().numpy()\n",
        "            axes[i, j].imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "            truth = classes[true_label] == classes[predicted.item()]\n",
        "            if truth:\n",
        "                color = \"g\"\n",
        "            else:\n",
        "                color = \"r\"\n",
        "\n",
        "            axes[i, j].set_title(f\"Truth: {classes[true_label]}\\n, Predicted: {classes[predicted.item()]}\", fontsize=10, c=color)\n",
        "            axes[i, j].axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "a1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
